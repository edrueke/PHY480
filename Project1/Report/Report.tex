\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage[margin=1.0in]{geometry}
\usepackage{enumitem}
\usepackage{listings}

\numberwithin{equation}{section}

\title{Programming a Linear Algebra Solution to the Poisson Equation}
\author{Elizabeth Drueke}

\begin{document}
\maketitle

\begin{abstract}
Poisson's equation is vital in the study of the physical world.  In every subfield of physics, from mechanics and quantum mechanics to electromagnetism, we are able to model physical systems subject to boundary conditions with this versatile equation.  This means that developing computer programs which can quickly and accurately solve this system is of the utmost importance.  Here, we develop a code using C++ compiled within the ROOT framework which can create a numerical approximation to the solution of the Poisson equation, specifically subject to the Dirichlet boundary conditions.  In the process, we also explore dynamic memory allocation in C++ and the use of classes.
\end{abstract}

\section{Introduction}
\label{sec:into}
It is important in physics to develop ways to deal with large quantities of data and to use that data to make close approximations of physical conditions.  In particular, we wish to develop computer programs which can both automate the process of numerical approximation and which can complete them in a timely manner.  One example of an equation which we will often need to solve when investigating physical situations is the Poisson equation, given by 

\begin{equation}
\label{eq:poisson}
-u''\left(x\right) = f\left(x\right),
\end{equation}

\noindent subject to the Dirichlet boundary conditions,

\begin{equation}
\label{eq:dirichlet}
u\left(0\right)=u\left(1\right)=0.
\end{equation}

In this report, we will study various numerical approximations of solutions to the Poisson equation in great detail, particularly focusing on using linear algebra and the solution of linear systems to determine a close approximation to the solution $u\left(x\right)$.  We pay special attention to the number of floating point operations (FLOPS) and the time required for various approximation methods.

\section{Theory}
\label{sec:theory}
The mathematics behind the solution to the Poisson equation presented here is mathematically rich in approximations.  From Eq.~\ref{eq:poisson}, we can see that we are required to compute the function $u\left(x\right)$ from its second derivative.  To do this, we note that we can always approximate the first derivative as

\begin{equation}
\label{eq:deriv1}
f'\left(x\right) \approx \frac{f\left(x+h\right)-f\left(x-h\right)}{2h},
\end{equation}

\noindent for $h<<1$ because the derivative is the slope of the line tangent to $f$ at that point.  This then implies that

$$\begin{array}{ccc} f''\left(x\right)&\approx&\frac{f'\left(x+h\right)-f'\left(x-h\right)}{2h} \\
&\approx&\frac{\frac{f\left(x+h+h\right)-f\left(x+h-h\right)}{2h} - \frac{f\left(x-h+h\right)-f\left(x-h-h\right)}{2h}}{2h} \\
&\approx&\frac{\frac{f\left(x+2h\right)-f\left(x\right)}{2h} - \frac{f\left(x\right)-f\left(x-2h\right)}{2h}}{2h} \\
&\approx&\frac{f\left(x+2h\right)-2f\left(x\right)+f\left(x-2h\right)}{\left(2h\right)^{2}}.\end{array}$$

\noindent Letting $2h\rightarrow h$, we then have

\begin{equation}
\label{eq:deriv2}
f''\left(x\right)\approx\frac{f\left(x+h\right)-2f\left(x\right)+f\left(x-h\right)}{h^{2}}.
\end{equation}

\noindent Eq.~\ref{eq:deriv2} will serve as our approximation to the second derivative throughout the remainder of this discussion.  
\\\indent Now, we have an expression which lends itself to the use of vectors.  Letting $$h = \frac{1}{n+1}$$ be the step size for some number of steps $n\in\mathbb{N}$, we see that we can treat this as a partition of the interval $\left[0,1\right]$, on which the Dirichlet conditions are valid.  Thus, we define each $x_{i}$ in the partition as $$x_{i}=ih,i=0,\ldots,n+1.$$  From these $x_{i}$ we can create a vector $\textbf{x}$\footnote{Note that throughout we indicate vectors as bold, lowercase letters (eg. $\textbf{x}$), and matrices as uppercase letters (eg. $A$).}

$$\textbf{x} = \left(0,\frac{1}{n+1},\frac{2}{n+1},\ldots,\frac{n}{n+1},1\right)^{T}.$$

\noindent Then, let $\textbf{b}$ be a vector of the function $f\left(x\right)$ evaluated at the points in $\textbf{x}$.  That is,

$$\begin{array}{ccc}\textbf{b} &=& \left(f\left(0\right),f\left(\frac{1}{n+1}\right),f\left(\frac{2}{n+1}\right),\ldots,f\left(\frac{n}{n+1}\right),f\left(1\right)\right)^{T} \\
&=& \left(0,f\left(\frac{1}{n+1}\right),f\left(\frac{2}{n+1}\right),\ldots,f\left(\frac{n}{n+1}\right),0\right)^{T}.
\end{array}$$

\noindent Now, we see that Eq.~\ref{eq:deriv2} lends itself nicely to the introduction of a linear algebra problem.  In particular, we can define an $\left(n+1\right)\times \left(n+1\right)$ matrix $A$ such that

\begin{equation}
\label{eq:a}
A = \frac{1}{h^{2}} \left(
\begin{array}{cccccc}
2 & -1 & 0 & \cdots & \cdots & 0 \\
-1 & 2 & -1 & 0 & \cdots & 0 \\
0 & -1 & 2 & -1 & 0 & \vdots \\
\vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
\vdots & \vdots & \vdots & \ddots & -1 & 2
\end{array}
\right)
\end{equation}

\noindent Then, we have that the Poisson Equation given by Eq.~\ref{eq:poisson} can be approximated as 

\begin{equation}
\label{eq:avb}
A\textbf{v}=\textbf{b}
\end{equation}

\noindent for some $\textbf{v}$ which represents $u\left(x\right)$ at various values of $x$.  And so we have a system of $n+1$ equations in $n+1$ unknowns.  Once we have solved for the vector $\textbf{v}$, we can use a plotting tool to plot the points and extrapolate a fit to the function.
\\\indent In general, there are two main ways in which we might solve  Eq.~\ref{eq:avb}, known as Gaussian elimination and LU Decomposition.  We will discuss these in general in Section~\ref{subsec:gausselim} and Section~\ref{subsec:ludecomp}, respectively, before discussing how we developed the computer algorithms for these methods specifically for our matrix $A$ given by Eq.~\ref{eq:a} in Section~\ref{sec:algorithm}.

\subsection{Gaussian Elimination}
\label{subsec:gausselim}

Gaussian elimination is the method of solving linear systems typically taught in an introductory linear algebra class. This method involves adding multiples of rows of the matrix to other rows in order to eliminate (or set to zero) off-diagonal elements.  In order to determine a solution, any row operation performed on the matrix $A$ must be performed on our solution vector $\textbf{b}$\footnote{Following the notation in Eq.~\ref{eq:avb}.} in parallel.  In most situations, it is necessary to employ both a forward and backward Gaussian elimination method in order to solve a full system.  To demonstrate this method, we look at a $3\times 3$ example.  The algorithm used within the code will be more explicitly discussed in Section~\ref{sec:algorithm}.
\\\indent Suppose we have a matrix $A$ of the form

\begin{equation}
\label{eq:ainit}
A = \left(
\begin{array}{ccc}
a_{11} & a_{12} & a_{12} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}
\right)
\end{equation}

\noindent and we wish to reduce it to row echelon form.  We might do this using Gaussian elimination.  To begin, we would use forward elimination to set the $a_{i1}$ components to zero for $i\neq1$.  Letting $R_{i}$ denote the $i^{th}$ row, we notice that letting 

\begin{equation}
\label{eq:rowops1}
\begin{align}
R_{2} & = R_{2} - \frac{a_{21}}{a_{11}}R_{1} \\
R_{3} & = R_{3} - \frac{a_{31}}{a_{11}}R_{1}
\end{align}
\end{equation}

\noindent should yield the desired outcome.  That is, we have 

$$A = \left(
\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}
\right) \rightarrow \left(
\begin{array}{ccc}
a_{11} & a_{12} & a_{12} \\
0 & a_{22} - \frac{a_{21}a_{12}}{a_{11}} & a_{23} - \frac{a_{21}a_{13}}{a_{11}} \\
0 & a_{32} - \frac{a_{31}a_{12}}{a_{11}} & a_{33} - \frac{a_{31}a_{13}}{a_{11}}
\end{array}\right).$$

\noindent Continuing in this manner, we will eventually have a matrix of the form 

\begin{equation}
A\prime = \left(
\begin{array}{ccc}
a_{11}\prime & a_{12}\prime & a_{13}\prime \\
0 & a_{22}\prime & a_{23}\prime \\
0 & 0 & a_{33}\prime
\end{array}\right),
\end{equation}

\noindent where the $\prime$ indicates that the element has been modified from its original definition in Eq.~\ref{eq:ainit}.  
\\\indent At this point, we may begin the backward Gaussian elimination process.  In the forward process, we were able to set all of the $a_{ij}=0$ for $i>j$.  In the backward process, we wish to do the same for the $a_{ij}$ with $i<j$.  In the end, we should have a pure diagonal matrix.
\\\indent To begin the backward process, we note that we can set the $a_{i3}$ elements to zero by similar calculations as those performed in Eq.~\ref{eq:rowops1}.  In particular, we can let 

\begin{equation}
\label{eq:rowops2}
\begin{align}
R_{2} &= R_{2} - \frac{a_{23}}{a_{33}}R_{3} \\
R_{1} &= R_{1} - \frac{a_{13}}{a_{33}}R_{3}
\end{align}
\end{equation}

\noindent Doing this, we see 

$$\left(
\begin{array}{ccc}
a_{11}\prime & a_{12}\prime & a_{13}\prime \\
0 & a_{22}\prime & a_{23}\prime \\
0 & 0 & a_{33}\prime
\end{array}\right) \rightarrow
\left(\begin{array}{ccc}
a_{11}\prime & a_{12}\prime & 0 \\
0 & a_{22}\prime & 0 \\
0 & 0 & a_{33}\prime 
\end{array}\right).$$

\noindent Continuing in this manner, we can see that we will eventually come across a pure diagonal matrix.  From here, having performed the row operations on the solution vector $\textbf{b}$ as well as the matrix $A$, we can find our solution $\textbf{v}$ by noting that 

\begin{equation}
v_{i} = \frac{\widetilde{b_{i}}}{\widetilde{a_{ii}}},
\end{equation}

\noindent where the $\sim$ indicates that this is the component of the fully Gaussian eliminated matrix/vector.  This procedure is easily generalized to work for any $n\times n$ matrix.
\\\indent One downside to this method of solving the system of linear equations is that whatever row operations are performed on $A$ in Eq.~\ref{eq:avb} must also be performed on $\textbf{b}$.  This means that the process must be repeated every time the solution vector is changed.  This restriction can be a serious time constraint on any program written to implement Gaussian elimination in order to solve systems of linear equations.  The time limitations of such an algorithm are discussed in Section~\ref{sec:algorithm}.

\subsection{LU Decomposition}
\label{subsec:ludecomp}

The second major linear system solving method is what is known as LU decomposition.  In this procedure, we decompose our $A$ matrix into a lower-triangular $L$ and an upper-triangular $U$ of the form

\begin{equation}
\label{eq:ludecomp1}
\begin{array}{cccc}
\left(
\begin{array}{ccc}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}
\right) &=&
\left(
\begin{array}{ccc}
1 & 0 & 0 \\
l_{21} & 1 & 0 \\
l_{31} & l_{32} & 1
\end{array}\right)&
\left(\begin{array}{ccc}
u_{11} & u_{12} & u_{13} \\
0 & u_{22} & u_{23} \\
0 & 0 & u_{33}
\end{array}\right) \\
A &=& L&U
\end{array}
\end{equation}

\noindent In contrast with the Gaussian elimination method, this method does not need to be repeated for every choice of solution vector $\textbf{b}$.  Instead, once $L$ and $U$ have been computed, they can be used to determine the solution $\textbf{v}$ for any solution vector $\textbf{b}$.
\\\indent As with the Gaussian elimination method, we present an example as an illustration of how the LU decomposition method works.  Assume we have some 

\begin{equation}
\label{eq:ainit2}
A = \left(\begin{array}{cccc}
a_{11} & a_{12} & a_{13} & a_{14} \\
a_{21} & a_{22} & a_{23} & a_{24} \\
a_{31} & a_{32} & a_{33} & a_{34} \\
a_{41} & a_{42} & a_{43} & a_{44}
\end{array}\right).
\end{equation}

\noindent Based on the multiplication as shown in Eq.~\ref{eq:ludecomp1}, we can see that we can directly solve for the $l_{ij}$ and $u_{ij}$.  In particular, in order, we see\footnote{Here, the boldface text indicates the unknown variable in each equation.}

\begin{equation}
\begin{align}
a_{11} =& {\bf u_{11}} &\Rightarrow \\
a_{21} =& {\bf l_{21}} u_{11} &\Rightarrow \\
a_{31} =& {\bf l_{31}}u_{11} &\Rightarrow \\
a_{41} =& {\bf l_{41}}u_{11} &\Rightarrow \\
a_{12} =& {\bf u_{12}} &\Rightarrow \\
a_{22} =& {\bf l_{21}}u_{12}+u_{22} &\Rightarrow \\
a_{32} =& l_{31}u_{12}+{\bf l_{32}}u_{22} &\Rightarrow \\
a_{42} =& l_{41}u_{12}+{\bf l_{42}}u_{22} &\Rightarrow \\
a_{13} =& {\bf u_{13}} &\Rightarrow \\
a_{23} =& l_{21}u_{13}+{\bf u_{23}} &\Rightarrow \\
a_{33} =& l_{31}u_{13}+l_{32}u_{23}+{\bf u_{33}} &\Rightarrow \\
a_{43} =& l_{42}u_{23}+{\bf l_{43}}u_{33}+l_{41}u_{13} &\Rightarrow \\
a_{14} =& {\bf u_{14}} &\Rightarrow \\
a_{24} =& l_{21}u_{14}+{\bf u_{24}} &\Rightarrow \\
a_{34} =& l_{31}u_{14}+l_{32}u_{24}+{\bf u_{34}} &\Rightarrow \\
a_{44} =& l_{41}u_{14}+l_{42}u_{24}+l_{43}u_{34}+{\bf u_{44}}. &
\end{align}
\end{equation}

\noindent This result generalizes to any $n\times n$ matrix.  In particular, an algorithm can be developed which will always give the $L$ and $U$ matrix from known values.  This algorithm, known as Doolittle's Algorithm \textbf{need citation}, proceeds as follows:

\begin{itemize}
\item Starting with column $j=1$, compute the first element by 

\begin{equation}
u_{1j}=a_{1j}
\end{equation}

\item For $i=2,\ldots,j-1$, compute

\begin{equation}
u_{ij}=a_{ij}-\sum_{k=1}^{i-1}l_{ik}u_{kj}
\end{equation}

\item Calculate the diagonal element as

\begin{equation}
u_{jj}=a_{jj}-\sum_{k-1}^{j-1} l_{jk}u_{kj}
\end{equation}

\item Calculate the $l_{ij}$ for $i>j$ as

\begin{equation}
l_{ij}=\frac{1}{u_{jj}}\left(a_{ij}-\sum_{k=1}^{j-1} l_{jk}u_{kj}\right)
\end{equation}

\item Repeat with columns $j=2,3,\ldots,n$.
\end{itemize}

\noindent Of course, this algorithm does not actually compute the solution to the linear system presented in Eq.~\ref{eq:avb}.  To compute the solution $\textbf{v}$, we must invoke another algorithm.  To begin, we note that

\begin{equation}
\label{eq:axbluxb}
A\textbf{v}=\textbf{b}\Rightarrow LU\textbf{v}=\textbf{b}.
\end{equation}

\noindent This implies that we should be able to define an intermediate vector $\textbf{y}=U\textbf{v}$ which can be easily computed.  In the four-dimensional example, we would have the set of equations

$$\begin{array}{rcc} u_{44}v_{4}&=&y_{4} \\
u_{33}v_{3}+u_{34}v_{4}&=&y_{3} \\
u_{22}v_{2}+u_{23}v_{3}+u_{24}v_{4}&=&y_{2} \\
u_{11}v_{1}+u_{12}v_{2}+u_{13}v_{3}+u_{14}v_{4}&=& y_{1} \end{array}$$

\noindent and

$$\begin{array}{rcc}y_{1}&=&b_{1} \\
l_{21}y_{1}+y_{2}&=&b_{2} \\
l_{31}y_{1}+l_{32}y_{2}+y_{3}&=&b_{3} \\
l_{41}y_{1}+l_{42}y_{2}+l_{43}y_{3}+y_{4}&=&b_{4}.\end{array}$$

\noindent Thus, there must be both an algorithm to decompose the matrix $A$ into an upper-triangular $U$ and a lower-triangular $L$ and another algorithm to actually compute the solution to Eq.~\ref{eq:avb} from that $L$ and $U$.
\\\indent One benefit to using LU decomposition over Gaussian elimination, aside from the fact that it will take less time than the standard Gaussian elimination method,\footnote{The specific argument to this statement is presented in Section~\ref{subsec:timedependence}.}  is that it is very natural to compute the determinant of the matrix once it has been LU-decomposed.  This is because we have 

$$A=LU \Rightarrow \det\left(A\right)=\det\left(LU\right)=\det\left(L\right)\det\left(U\right),$$

\noindent and the determinant of either an upper-triangular or lower-triangular matrix is simply the product of its diagonal elements.  Because $L$ has 1's along its diagonal, we can see that

\begin{equation}
\det\left(A\right)=\prod_{i=1}^{n}u_{ii}.
\end{equation}

\section{The Algorithm}
\label{sec:algorithm}

The meat of this project is the development of an algorithm which can compute a fair approximation for $u\left(x\right)$ in Eq.~\ref{eq:poisson}.  In order to check our solutions, we take our function $f$ to be given by $f\left(x\right) = 100e^{-10x}$.  We work with the linear algebra solutions described in Section~\ref{sec:theory}.  To do this, we first define our matrix $A$ as in Eq.~\ref{eq:a} and a vector of solutions $\textbf{b}$ as in Eq.~\ref{eq:avb}.  Noting that, in fact, we know the solution for $u\left(x\right)$ at $x=0$ and $x=1$\footnote{These values are given by the Dirichlet boundary conditions.}, we do not include either the $x_{0}$ or $x_{n+1}$\footnote{In the notation of $x_{i}=ih,i=0,\ldots,n+1$ where $h=\frac{1}{n+1}$.} terms.  However, when we perform the fit to our plot at the end, these boundary conditions will be included for completeness.  To declare these objects, we create classes $\texttt{themat}$ and $\texttt{thevec}$ for matrices and vectors handled with dynamic memory. 
\\\indent The classes and function definitions come in documents $\texttt{classes.h}$ and $\texttt{classes.C}$.  Every object of the $\texttt{thevec}$ class has associated with it an integer $\texttt{sz}$ which is the number of rows of the vector, and a pointer to a dynamic memory array $\texttt{point}$ in which we can place the values of the components of the vector.  This class comes equipped with several constructors, a destructor (which de-allocates any memory in use by the object), several overloaded operators (addition of two vectors, subtraction of two vectors, multiplication of two vectors (in the form of a dot product), etc.), a component retrieval function ($\texttt{operator[]}$) which returns the value of the vector at a particular index, and a printing member function.  
\\\indent The setup of the $\texttt{themat}$ class is very similar to that of the $\texttt{thevec}$ class.  The $\texttt{sz}$ component here is the $n$ which defines the matrix (ie. creates an $n\times n$ matrix) and the $\texttt{point}$ is a double pointer.  This class also comes with multiple constructors, several overloaded operators (including matrix-matrix and matrix-vector multiplication), a component-retrieval function, and a print function.  For ease of use, $\texttt{thevec}$ was declared as a friend function of $\texttt{themat}$.  
\\\indent Once the vector and matrix objects are defined, it is just a matter of solving Eq.~\ref{eq:avb} using the matrix and vector specific to the problem.  However, because we are working with a specific kind of matrix, there are several things we can do to reduce the computing power needed to solve the problem.  The first is noting that we do not have to proceed with the full LU-decomposition in order to decompose our special matrix.  Looking at a $4\times4$ example, we see a pattern forming. 

\begin{equation}
\begin{array}{cccc}
\left(\begin{array}{cccc}
1 & 0 & 0 & 0 \\
-\frac{1}{2} & 1 & 0 & 0 \\
0 & -\frac{2}{3} & 1 & 0 \\
0 & 0 & -\frac{3}{4} & 1 
\end{array}\right) &
\left(\begin{array}{cccc}
2 & -1 & 0 & 0 \\
0 & \frac{3}{2} & -1 & 0 \\
0 & 0 & \frac{4}{3} & -1 \\
0 & 0 & 0 & \frac{5}{4} 
\end{array}\right) &=&
\left(\begin{array}{cccc}
2 & -1 & 0 & 0 \\
-1 & 2 & -1 & 0 \\
0 & -1 & 2 & -1 \\
0 & 0 & -1 & 2
\end{array}\right) \\
L & U & = & A.
\end{array}
\end{equation}

\noindent In general, then, we might suppose that the matrix elements $l_{ij}$ and $u_{ij}$ of $L$ and $U$ will take the form

\begin{equation}
\begin{array}{cc}
l_{ii} = 1, & l_{i,i-1} = -\frac{i-1}{i-2}, \\
u_{ii} = \frac{i+1}{i}, & u_{i,i+1}=-1
\end{array}
\end{equation}

\noindent and all other $l_{ij}$ and $u_{ij}$ are 0.  Noticing this means that we are required to perform far fewer float operations in order to decompose the matrix.  
\\\indent From there, it becomes simpler to solve the linear system as well.  Referring back to the $4\times4$ example, we can now solve for our intermediate $\textbf{y}$ and then our solution $\textbf{v}$ with the following equations:

\begin{equation}
\label{eq:easylusolver}
\begin{array}{ccc}
\begin{array}{c}
y_{1}=b_{1} \\
-\frac{1}{2}y_{1}+y_{2}=b_{2} \\
-\frac{2}{3}y_{2}+y_{3}=b_{3} \\
-\frac{3}{4}y_{3}+y_{4}=b_{4} \\
-\frac{4}{5}y_{4}+y_{5}=b_{5}
\end{array} & \rightarrow &
\begin{array}{c}
2v_{1}-v_{2}=y_{1} \\
\frac{3}{2}v_{2}-v_{3}=y_{2} \\
\frac{4}{3}v_{3}-v_{4}=y_{3} \\
\frac{5}{4}v_{4}-v_{5}=y_{4} \\
\frac{6}{5}v_{5}=y_{5}
\end{array}
\end{array}
\end{equation}

\noindent In fact, with the matrix as specific as it is, it isn't necessary even to pass the $L$ and $U$ matrices to the solving algorithm in order to solve the system.  Instead, one possible version of the algorithm is:

\begin{1stlisting}
\texttt{y[0]=b[0]; \\ \indent for(int i=1;i<n;i++)\{ \\ \indent\indent y[i]=b[i]+(i/(i+1))*y[i-1]; \\\indent \} \\\indent v[n-1] = n*y[n-1]/(n+1); \\ \indent
for(int i=n-2;i>-1;i--)\{ \\\indent \indent v[i]=((i+1)/(i+2))*(y[i]+v[i+1]); \\\indent \}}
\end{1stlisting}

\noindent However, the main goal in rewriting the algorithm specific to the matrix required to solve the Poisson equation is to reduce the number of FLOPs, and writing the program in this way will actually force the computer to perform unnecessary floating point operations because the fractional factors will have already been computed in the LU decomposition step.  It is for this reason that we create an essentially shorter version of the LU decomposition solving function, which works with the already-computed elements of the $L$ and $U$ matrices.  In particular, our algorithm first solves for the intermediate vector $\textbf{y}$ as 

\begin{1stlisting}
\texttt{y[0]=b[0]; \\\indent for(int i=0;i<n;i++)\{ \\\indent\indent y[i]=b[i]-L[i][i-1]*y[i-1]; \\\indent\}}
\end{1stlisting}

\noindent and then solves for the solution to the problem $\textbf{v}$ as

\begin{1stlisting}
\texttt{v[n-1]=y[n-1]/U[n-1][n-1]; \\\indent for(int i=n-2;i>-1;i--)\{ \\\indent\indent v[i]=(y[i]+v[i+1])/U[i][i];\\\indent\}}
\end{1stlisting}

\noindent In this way, we are able to reasonably minimize the number of FLOPs required for the calculation.  A full analysis of the expected and observed time dependences of these algorithms is presented in Section~\ref{subsec:timedependence} and Section~\ref{sec:results}.

\subsection{Time Dependences}
\label{subsec:timedependence}

As mentioned in Section~\ref{sec:algorithm}, we wish to develop an algorithm which is not only able to solve numerically the Poisson equation, but which can also do so within a reasonable time frame and with limited computer memory usage. \textbf{need more here - what are the expected time dependences?}.

\section{Results and Benchmarks}
\label{sec:results}

Before entering a discussion of the full results of the program developed, it is important to validate the code works properly.  In particular, it is reasonable to assume that, if all three algorithms written predict the same solution vector, then the solution is correct.  Such an analysis is performed in the $\texttt{benchmarks.C}$ file.  In this file, a matrix of the special form is created with varying dimensions between $1\times1$ and $50\times50$ and the resulting solution vector $\textbf{v}$ is computed using the full Gaussian elimination method ($\texttt{gauss\_elim()}$), the full LU decomposition solver ($\texttt{LU\_decomp()}$ together with $\texttt{LU\_decomp\_solver()}$), and the special LU decomposition solver which will work only on matrices of our specific form ($\texttt{LU\_decomp\_special()}$ together with $\texttt{LU\_decomp\_solver\_special()}$).  The results of these tests are found in $\texttt{benchmarks.txt}$.  It is shown here that, for a specific $A$, all three algorithms return precisely the same solution vector $\textbf{v}$.  In addition, the $L$ and $U$ matrices created by the different LU Decomposition algorithms are also identical and, when multiplied together, yield our original matrix $A$.
\\\indent We now discuss the results of the analysis.  Because of the simplicity of our test function, $f\left(x\right)=100e^{-10x}$, we can directly compute $u\left(x\right)$ either by hand or with any computing software.  We chose to use WolframAlpha \textbf{need citation}.  We find that, subject to the Dirichlet boundary conditions, the solution is given by 

\begin{equation}
\label{eq:expectsol}
u\left(x\right)=\frac{x}{e^{10}}-x-e^{-10x}+1
\end{equation}

\\\indent The results for various numbers of steps are plotted in \textbf{need figure}.  From the image, it appears that increasing the number of steps improves the validity of the result.  In particular, we are able to compute the relative error $\epsilon_{i}$ between the expected value $u_{exp}$ of the function at point $x_{i}$ and the result computed by our special LU decomposition algorithm $u_{comp}$ as 

\begin{equation}
\label{eq:relerror}
\epsilon_{i} = \log_{10}\left(|\frac{u_{comp}-u_{exp}}{u_{exp}}|\right).
\end{equation}

\noindent The maximum value of $\epsilon_{i}$ for each step value investigated is listed in Table \textbf{need table (and maybe plot?)}.  This confirms our suspicions that the error decreases with increasing number of steps.
\\\textbf{time required}

\section{Conclusions}
\label{sec:conclusions}

Something about how the class isn't complete actually.  Errors.



\end{document}